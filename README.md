# <div align="center">HierCon: Hierarchical Contrastive Attention</br>for Audio Deepfake Detection</div>

### <div align="center">[Nicholas Liang](https://github.com/nicholas-liang), [Soyeon Caren Han](https://scholar.google.com/citations?user=VH2jVOgAAAAJ), Mike Wang, [Christopher Leckie](https://scholar.google.com/citations?user=0vPWOQcAAAAJ)</div>
#### <div align="center">Accepted at The Web Conference 2026 (WWW'26)

**Abstract:** Audio deepfakes generated by modern TTS and voice conversion systems are increasingly difficult to distinguish from real speech, raising serious risks for security and online trust. While state-of-the-art self-supervised models provide rich multi-layer representations, existing detectors treat layers independently and overlook temporal and hierarchical dependencies critical for identifying synthetic artefacts. We propose HierCon, a hierarchical layer attention framework combined with margin-based contrastive learning that models dependencies across temporal frames, neighbouring layers, and layer groups, while encouraging domain-invariant embeddings. Evaluated on ASVspoof 2021 DF and In-the-Wild datasets, our method achieves state-of-the-art performance (1.93% and 6.87% EER), improving over independent layer weighting by 36.6% and 22.5% respectively. The results and attention visualisations confirm that hierarchical modelling enhances generalisation to cross-domain generation techniques and recording conditions.

<p align="center">
  <img alt="Overall Architecture" src="architecture_diagram.drawio.png" width="800" />
</p>

----

## Key Features

- **Hierarchical Attention Framework**: Three-stage attention architecture modeling temporal, intra-group, and inter-group dependencies across XLS-R layers
- **Contrastive Learning**: Margin-based contrastive regularization for domain-invariant representations
- **State-of-the-Art Performance**: Achieves 1.93% EER on ASVspoof 2021 DF and 6.87% on In-the-Wild benchmarks
- **Interpretability**: Attention visualizations reveal which temporal frames and layer groups drive predictions

## Datasets

We evaluate HierCon on three benchmark datasets:

- **ASVspoof 2019 LA** (training): Logical access subset for training
- **ASVspoof 2021 LA** (evaluation): 148,176 utterances 
- **ASVspoof 2021 DF** (evaluation): 533,928 utterances spanning over 100 deepfake generation pipelines
- **In-the-Wild (ITW)** (evaluation): 31,779 real-world samples

Please download the datasets from the [ASVspoof 2021 challenge](https://www.asvspoof.org/) and organize them according to the official structure.

## Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/hiercon.git
cd hiercon

# Create conda environment
conda create -n hiercon python=3.8
conda activate hiercon

# Install dependencies
pip install -r requirements.txt
```

### Requirements

```
torch>=1.10.0
torchaudio>=0.10.0
transformers>=4.20.0
numpy>=1.21.0
scipy>=1.7.0
scikit-learn>=1.0.0
```

## Model Architecture

HierCon consists of three main components:

1. **XLS-R Feature Extraction**: Extracts 24-layer transformer representations from 4-second audio windows
2. **Hierarchical Attention Module**: 
   - **Stage 1 - Temporal Attention**: Focuses on informative frames within each layer
   - **Stage 2 - Intra-Group Attention**: Captures dependencies among neighboring layers (8 groups × 3 layers)
   - **Stage 3 - Inter-Group Attention**: Integrates evidence across layer groups
3. **Contrastive Learning**: Margin-based loss enforcing geometric constraints on embeddings

## Training

### Basic Training

```bash
python train.py --dataset asvspoof2019 \
                --model_name hiercon \
                --batch_size 16 \
                --lr 1e-6 \
                --epochs 50 \
                --lambda_con 0.1 \
                --margin 0.2
```

### Parameters

```
--dataset         Dataset name (asvspoof2019, asvspoof2021)
--model_name      Model architecture (hiercon, baseline)
--batch_size      Training batch size (default: 16)
--lr              Learning rate (default: 1e-6)
--epochs          Number of training epochs (default: 50)
--lambda_con      Contrastive loss weight (default: 0.1)
--margin          Contrastive margin (default: 0.2)
--num_groups      Number of layer groups (default: 8)
--group_size      Layers per group (default: 3)
--use_augment     Use RawBoost augmentation (default: True)
--seed            Random seed for reproducibility
```

### Training with RawBoost Augmentation

RawBoost augmentation is applied by default. To disable:

```bash
python train.py --dataset asvspoof2019 --use_augment False
```

## Evaluation

### Evaluate on Test Sets

```bash
python evaluate.py --checkpoint ./checkpoints/best_model.pt \
                   --test_dataset df \
                   --output_dir ./results
```

Available test datasets: `la`, `df`, `itw`

### Reproduce Paper Results

```bash
# Download pretrained models
wget https://example.com/hiercon_pretrained.zip
unzip hiercon_pretrained.zip -d ./checkpoints

# Evaluate on all benchmarks
bash evaluate_all.sh
```

## Inference

Run inference on custom audio files:

```bash
python inference.py --checkpoint ./checkpoints/best_model.pt \
                    --audio_path /path/to/audio.wav \
                    --output_dir ./predictions
```

The script outputs:
- Prediction score (0=bonafide, 1=spoof)
- Attention visualizations (temporal, intra-group, inter-group)

## Results

### Overall Performance

| Model | 21 LA | 21 DF | ITW |
|-------|-------|-------|-----|
| Wav2Vec+AASIST | 0.82 | 2.85 | - |
| OCKD | 0.90 | 2.27 | 7.86 |
| XLS-R + SLS | 3.88 | 2.09 | 8.87 |
| **XLS-R + HierCon (Ours)** | **2.46** | **1.93** | **6.87** |

*All results reported as Equal Error Rate (EER %) - lower is better*

### Ablation Study

| Configuration | 21 LA | 21 DF | ITW |
|---------------|-------|-------|-----|
| XLS-R + SLS (Baseline) | 3.88 | 2.09 | 8.87 |
| + Hierarchical Attention | 2.97 | 2.13 | 8.81 |
| + Hier. Attn + Contrastive | **2.46** | **1.93** | **6.87** |

### Attention Visualization

The hierarchical attention mechanism provides interpretability:
- **Temporal attention** concentrates on middle regions (40-70% duration)
- **Intra-group attention** shows adaptive layer selection across groups
- **Inter-group attention** reveals mid-level dominance (layers 12-14)

## Project Structure

```
hiercon/
├── data/
│   ├── asvspoof2019/
│   └── asvspoof2021/
├── models/
│   ├── hiercon.py
│   ├── attention.py
│   └── contrastive.py
├── utils/
│   ├── dataloader.py
│   ├── augmentation.py
│   └── metrics.py
├── train.py
├── evaluate.py
├── inference.py
└── README.md
```

## Citation

```bibtex
@inproceedings{liang2026hiercon,
    author = {Liang, Nicholas and Han, Soyeon Caren and Wang, Mike and Leckie, Christopher},
    title = {HierCon: Hierarchical Contrastive Attention for Audio Deepfake Detection},
    year = {2026},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3774904.3792873},
    doi = {10.1145/3774904.3792873},
    booktitle = {Proceedings of the ACM on Web Conference 2026},
    location = {Dubai, United Arab Emirates},
    series = {WWW '26}
}
```

## Acknowledgments

This research was supported by:
- Korea Planning & Evaluation Institute of Industrial Technology (KEIT) funded by the Ministry of Trade, Industry and Energy (No.RS-2025-25458052, Development of Core Technologies for Manufacturing Foundation Models)
- Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No.RS-2025-02217259, Development of self-evolving AI bias detection-correction-explain platform based on international multidisciplinary governance)

## Contact

For questions or issues, please open an issue on GitHub or contact:
- Nicholas Liang: nnliang@unimelb.edu.au
- Soyeon Caren Han: caren.han@unimelb.edu.au
